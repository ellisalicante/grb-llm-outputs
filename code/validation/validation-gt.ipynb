{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of the gender representation bias quantification method\n",
    "\n",
    "Validate the LLM-based gender representation bias quantification method on an annotated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Param\n",
    "dataset_name = \"stories_val\"\n",
    "exp_name = \"val\"\n",
    "\n",
    "# Generator LLM\n",
    "gen_model = \"claude-3-7-sonnet-20250219\"\n",
    "\n",
    "# Evaluator LLM\n",
    "#eval_model = \"gpt-4o-2024-08-06\"\n",
    "#eval_model = \"gpt-4o-2024-11-20\"\n",
    "eval_model = \"gpt-4.1-2025-04-14\"\n",
    "#eval_model = \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\"\n",
    "#eval_model = \"deepseek-ai/DeepSeek-V3\"\n",
    "\n",
    "#lang = \"cs\"\n",
    "lang = \"sl\"\n",
    "\n",
    "run = 1\n",
    "\n",
    "gt_pathname = f\"../../data/validation/{lang}.json\" # ground truth dataset\n",
    "model_output_pathname = f\"../../grb/{dataset_name}/{exp_name}/{gen_model}/{lang}/{eval_model}/{run}/000000.json\" # model output\n",
    "evaluated_model_output_pathname = model_output_pathname.replace(\"000000.json\", \"evaluated.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON files\n",
    "with open(gt_pathname, encoding=\"utf-8\") as f:\n",
    "    gt_data = json.load(f)\n",
    "\n",
    "with open(model_output_pathname, encoding=\"utf-8\") as f:\n",
    "    model_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract GRB arrays from both files\n",
    "gt_grb = gt_data.get(\"grb\", [])\n",
    "model_grb = model_data.get(\"grb\", [])\n",
    "\n",
    "# Initialize global counters\n",
    "TP_total = 0\n",
    "FP_total = 0\n",
    "FN_total = 0\n",
    "exact_match_count = 0\n",
    "\n",
    "# Evaluate each sentence\n",
    "for gt_item, model_item in zip(gt_grb, model_grb):\n",
    "    gt_analysis = gt_item[\"analysis\"]\n",
    "    model_analysis = model_item[\"analysis\"]\n",
    "\n",
    "    gt_counts = Counter((entry[\"word\"].lower(), entry[\"class\"].upper()) for entry in gt_analysis)\n",
    "    model_counts = Counter((entry[\"word\"].lower(), entry[\"class\"].upper()) for entry in model_analysis)\n",
    "\n",
    "    TP = sum(min(gt_counts[k], model_counts[k]) for k in gt_counts if k in model_counts)\n",
    "\n",
    "    FP = 0\n",
    "    extra_words = []\n",
    "    for k, count in model_counts.items():\n",
    "        if k not in gt_counts:\n",
    "            FP += count\n",
    "            extra_words.extend([{\"word\": k[0], \"class\": k[1]}] * count)\n",
    "        else:\n",
    "            diff = count - gt_counts[k]\n",
    "            if diff > 0:\n",
    "                FP += diff\n",
    "                extra_words.extend([{\"word\": k[0], \"class\": k[1]}] * diff)\n",
    "\n",
    "    FN = 0\n",
    "    missing_words = []\n",
    "    for k, count in gt_counts.items():\n",
    "        if k not in model_counts:\n",
    "            FN += count\n",
    "            missing_words.extend([{\"word\": k[0], \"class\": k[1]}] * count)\n",
    "        else:\n",
    "            diff = count - model_counts[k]\n",
    "            if diff > 0:\n",
    "                FN += diff\n",
    "                missing_words.extend([{\"word\": k[0], \"class\": k[1]}] * diff)\n",
    "\n",
    "    misclassified = []\n",
    "    model_word_map = defaultdict(list)\n",
    "    for (word, c) in model_counts:\n",
    "        model_word_map[word].append(c)\n",
    "\n",
    "    for (word, gt_class), gt_count in gt_counts.items():\n",
    "        if word in model_word_map and gt_class not in model_word_map[word]:\n",
    "            model_wrong_classes = [c for c in model_word_map[word] if c != gt_class]\n",
    "            for wrong_class in model_wrong_classes:\n",
    "                mismatch_count = min(gt_counts[(word, gt_class)], model_counts[(word, wrong_class)])\n",
    "                misclassified.extend([{\"word\": word, \"class\": wrong_class}] * mismatch_count)\n",
    "\n",
    "    # Attach sentence-level evaluation\n",
    "    model_item[\"evaluation\"] = {\n",
    "        \"TP\": TP,\n",
    "        \"FP\": FP,\n",
    "        \"FN\": FN,\n",
    "        \"missing_words\": missing_words,\n",
    "        \"extra_words\": extra_words,\n",
    "        \"misclassified_words\": misclassified\n",
    "    }\n",
    "\n",
    "    TP_total += TP\n",
    "    FP_total += FP\n",
    "    FN_total += FN\n",
    "\n",
    "    if FP == 0 and FN == 0 and not misclassified:\n",
    "        exact_match_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final stats\n",
    "precision = TP_total / (TP_total + FP_total) if (TP_total + FP_total) > 0 else 0.0\n",
    "recall = TP_total / (TP_total + FN_total) if (TP_total + FN_total) > 0 else 0.0\n",
    "f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "exact_match_rate = exact_match_count / len(gt_grb) if gt_grb else 0.0\n",
    "\n",
    "# Save stats in the top-level structure\n",
    "model_data[\"stats\"] = {\n",
    "    \"TP\": TP_total,\n",
    "    \"FP\": FP_total,\n",
    "    \"FN\": FN_total,\n",
    "    \"Precision\": round(precision, 4),\n",
    "    \"Recall\": round(recall, 4),\n",
    "    \"F1\": round(f1_score, 4),\n",
    "    \"Exact_Match_Rate\": round(exact_match_rate, 4)\n",
    "}\n",
    "\n",
    "# Save output\n",
    "output_path = Path(evaluated_model_output_pathname)\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(model_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Evaluation completed. Results saved to {output_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
